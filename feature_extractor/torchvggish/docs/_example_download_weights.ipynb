{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook demonstrates how to replicate converting tensorflow\n",
    "weights from tensorflow's vggish to torchvggish\n",
    "\"\"\" \n",
    "\n",
    "# Download the audioset directory using subversion\n",
    "# !apt-get -qq install subversion   # uncomment if on linux\n",
    "!svn checkout https://github.com/tensorflow/models/trunk/research/audioset\n",
    "\n",
    "# Download audioset requirements\n",
    "!pip install numpy scipy\n",
    "!pip install resampy tensorflow six soundfile\n",
    "\n",
    "# grab the VGGish model checkpoints & PCA params\n",
    "!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "\n",
      "Testing your install of VGGish\n",
      "\n",
      "Log Mel Spectrogram example:  [[-4.47297436 -4.29457354 -4.14940631 ... -3.9747003  -3.94774997\n",
      "  -3.78687669]\n",
      " [-4.48589533 -4.28825497 -4.139964   ... -3.98368686 -3.94976505\n",
      "  -3.7951698 ]\n",
      " [-4.46158065 -4.29329706 -4.14905953 ... -3.96442484 -3.94895483\n",
      "  -3.78619839]\n",
      " ...\n",
      " [-4.46152626 -4.29365061 -4.14848608 ... -3.96638113 -3.95057575\n",
      "  -3.78538167]\n",
      " [-4.46152595 -4.2936572  -4.14848104 ... -3.96640507 -3.95059567\n",
      "  -3.78537143]\n",
      " [-4.46152565 -4.29366386 -4.14847603 ... -3.96642906 -3.95061564\n",
      "  -3.78536116]]\n",
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/harrisontaylor/.conda/envs/audioset-experiments/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "VGGish embedding:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.16137293 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.80695796\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.36792755 0.03582409 0.         0.         0.\n",
      " 0.         0.38027024 0.1375593  0.9174708  0.8065634  0.\n",
      " 0.         0.         0.         0.04036281 0.7076243  0.\n",
      " 0.497839   0.24081808 0.21565434 0.88492286 1.19568    0.6706197\n",
      " 0.20779458 0.01639861 0.17471863 0.         0.         0.25100806\n",
      " 0.         0.         0.14607918 0.         0.39887053 0.30542105\n",
      " 0.12896761 0.         0.         0.         0.         0.\n",
      " 0.5385133  0.         0.         0.04941072 0.42527416 0.18537284\n",
      " 0.         0.         0.14753515 0.         0.         0.69933873\n",
      " 0.45541188 0.05174822 0.         0.01992539 0.         0.\n",
      " 0.5181578  0.565576   0.6587975  0.         0.         0.41056332\n",
      " 0.         0.         0.         0.25765193 0.23232114 0.24026448\n",
      " 0.         0.         0.         0.         0.         0.26523757\n",
      " 0.         0.48460823 0.         0.         0.19325787 0.\n",
      " 0.20123348 0.         0.03368621 0.         0.         0.\n",
      " 0.         0.17836356 0.024749   0.06889972 0.         0.\n",
      " 0.         0.08246281 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Postprocessed VGGish embedding:  [169  10 154 127 191  66 124  69 157 232 142  21 128 131  43   3  33 111\n",
      " 198 153  76 255 194  60  71 179 146 131 167  60  79  76 192  84 102 160\n",
      "  23  91 173  13 149 186 115 202 252 163  84 145 107 255   5 198  81   0\n",
      " 203 110  35 104 101 131 255   0   0 158 136  74 115 152  77 154  54 151\n",
      "  82 243  57 116 165 153  85 181 152   0 255 122  29 255  46 105 110  43\n",
      "   0  90  58  13 255 108  96 255  84 121 255  75 176 111 176  64  83 231\n",
      " 255  82 255  94  81 144  99 173 255   0   0 158  31 230 112 255   0 255\n",
      "  20 255]\n",
      "\n",
      "Looks Good To Me!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test install\n",
    "!mv audioset/* .\n",
    "from vggish_smoke_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "vggish/conv1/weights:0\n",
      "\t(3, 3, 1, 64)\n",
      "vggish/conv1/biases:0\n",
      "\t(64,)\n",
      "vggish/conv2/weights:0\n",
      "\t(3, 3, 64, 128)\n",
      "vggish/conv2/biases:0\n",
      "\t(128,)\n",
      "vggish/conv3/conv3_1/weights:0\n",
      "\t(3, 3, 128, 256)\n",
      "vggish/conv3/conv3_1/biases:0\n",
      "\t(256,)\n",
      "vggish/conv3/conv3_2/weights:0\n",
      "\t(3, 3, 256, 256)\n",
      "vggish/conv3/conv3_2/biases:0\n",
      "\t(256,)\n",
      "vggish/conv4/conv4_1/weights:0\n",
      "\t(3, 3, 256, 512)\n",
      "vggish/conv4/conv4_1/biases:0\n",
      "\t(512,)\n",
      "vggish/conv4/conv4_2/weights:0\n",
      "\t(3, 3, 512, 512)\n",
      "vggish/conv4/conv4_2/biases:0\n",
      "\t(512,)\n",
      "vggish/fc1/fc1_1/weights:0\n",
      "\t(12288, 4096)\n",
      "vggish/fc1/fc1_1/biases:0\n",
      "\t(4096,)\n",
      "vggish/fc1/fc1_2/weights:0\n",
      "\t(4096, 4096)\n",
      "vggish/fc1/fc1_2/biases:0\n",
      "\t(4096,)\n",
      "vggish/fc2/weights:0\n",
      "\t(4096, 128)\n",
      "vggish/fc2/biases:0\n",
      "\t(128,)\n",
      "values written to vggish_dict\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import vggish_slim\n",
    "\n",
    "vggish_dict = {}\n",
    "# load the model and get info \n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    vggish_slim.define_vggish_slim(training=True)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess,\"vggish_model.ckpt\")\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "\n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(\"%s\" % (var.name))\n",
    "        print(\"\\t\" + str(var.shape))\n",
    "        vggish_dict[var.name] = val\n",
    "    print(\"values written to vggish_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define torch model for vggish\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# From vggish_slim:\n",
    "# The VGG stack of alternating convolutions and max-pools.\n",
    "#     net = slim.conv2d(net, 64, scope='conv1')\n",
    "#     net = slim.max_pool2d(net, scope='pool1')\n",
    "#     net = slim.conv2d(net, 128, scope='conv2')\n",
    "#     net = slim.max_pool2d(net, scope='pool2')\n",
    "#     net = slim.repeat(net, 2, slim.conv2d, 256, scope='conv3')\n",
    "#     net = slim.max_pool2d(net, scope='pool3')\n",
    "#     net = slim.repeat(net, 2, slim.conv2d, 512, scope='conv4')\n",
    "#     net = slim.max_pool2d(net, scope='pool4')\n",
    "#     # Flatten before entering fully-connected layers\n",
    "#     net = slim.flatten(net)\n",
    "#     net = slim.repeat(net, 2, slim.fully_connected, 4096, scope='fc1')\n",
    "#     # The embedding layer.\n",
    "#     net = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2')\n",
    "\n",
    "vggish_list = list(vggish_dict.values())\n",
    "def param_generator():\n",
    "    param = vggish_list.pop(0)\n",
    "    transposed = np.transpose(param)\n",
    "    to_torch = torch.from_numpy(transposed)\n",
    "    result = torch.nn.Parameter(to_torch)\n",
    "    yield result\n",
    "\n",
    "class VGGish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Linear(512*24, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # extract weights from `vggish_list`\n",
    "        for seq in (self.features, self.embeddings):\n",
    "            for layer in seq:\n",
    "                if type(layer).__name__ != \"MaxPool2d\" and type(layer).__name__ != \"ReLU\":\n",
    "                    layer.weight = next(param_generator())\n",
    "                    layer.bias = next(param_generator())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.embeddings(x)\n",
    "        return x\n",
    "\n",
    "net = VGGish()\n",
    "net.eval()\n",
    "\n",
    "# Save weights to disk\n",
    "torch.save(net.state_dict(), \"./vggish.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
